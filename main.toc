\contentsline {section}{\numberline {1}Useful Notation Reminders}{5}
\contentsline {section}{\numberline {2}Chapter 1: An introduction to important concepts in statistical learning - Edition 1}{6}
\contentsline {subsection}{\numberline {2.1}Important Concepts and Definitions}{6}
\contentsline {subsection}{\numberline {2.2}Goodness of Fit and Brownian Bridge:}{6}
\contentsline {subsection}{\numberline {2.3}Minimum Distance Estimation}{6}
\contentsline {subsection}{\numberline {2.4}Convergence}{6}
\contentsline {subsection}{\numberline {2.5}Permutation Testing}{7}
\contentsline {subsubsection}{\numberline {2.5.1}Fisher's Permutation Test Summary:}{7}
\contentsline {subsubsection}{\numberline {2.5.2}Choosing B (number of permutations to do):}{7}
\contentsline {subsection}{\numberline {2.6}Irregular Parameters}{7}
\contentsline {subsection}{\numberline {2.7}Stein Estimation}{8}
\contentsline {section}{\numberline {3}Chapter 1: An introduction to important concepts in statistical learning - Edition 2}{9}
\contentsline {subsection}{\numberline {3.1}Important Concepts and Definitions}{9}
\contentsline {subsection}{\numberline {3.2}Decision Theory Framework}{9}
\contentsline {subsection}{\numberline {3.3}Ways of Comparing Decision Procedures - Based on risk}{10}
\contentsline {subsection}{\numberline {3.4}Sufficient Statistics, Rao-Blackwellization, and Neyman-Pearson Factorization}{10}
\contentsline {subsection}{\numberline {3.5}Example Problems and Solutions - Chapter One}{11}
\contentsline {subsubsection}{\numberline {3.5.1}Bayes estimator for Bernoulli Trials}{11}
\contentsline {subsubsection}{\numberline {3.5.2}Minimal Sufficiency Derived from Neyman-Pearson Factorization Theorem}{13}
\contentsline {subsubsection}{\numberline {3.5.3}The Order Statistics are Sufficient}{14}
\contentsline {subsubsection}{\numberline {3.5.4}The Order Statistics are Equivalent to the Empirical CDF}{14}
\contentsline {subsubsection}{\numberline {3.5.5}The Minimal Sufficient Statistic for a Laplace Model}{15}
\contentsline {subsubsection}{\numberline {3.5.6}Explanation: Suffficiency is important in the Rao-Blackwell Theorem}{15}
\contentsline {section}{\numberline {4}Chapter 2: Methods of Estimation}{16}
\contentsline {subsection}{\numberline {4.1}Heuristics in Estimations}{16}
\contentsline {subsubsection}{\numberline {4.1.1}Examples}{16}
\contentsline {subsection}{\numberline {4.2}Plug-in and Extension Principle}{17}
\contentsline {subsubsection}{\numberline {4.2.1}Plug-in Principle}{18}
\contentsline {subsubsection}{\numberline {4.2.2}Extension Principle}{18}
\contentsline {subsubsection}{\numberline {4.2.3}Examples of Plug-in and Extensions}{18}
\contentsline {subsection}{\numberline {4.3}Minimum Contrast Estimates}{18}
\contentsline {subsubsection}{\numberline {4.3.1}MLE as Minimum Contrast Estimates from the Kullback-Leibler Divergence}{18}
\contentsline {subsubsection}{\numberline {4.3.2}MLE as Estimating Equations}{19}
\contentsline {subsection}{\numberline {4.4}Maximum Likelihood in Exponential Families}{19}
\contentsline {subsection}{\numberline {4.5}Making sense of Plug-in estimates, Minimum Contrast estimates and Maximum Likelihood estimate}{19}
\contentsline {section}{\numberline {5}Chapter 3: Measure of Performance and "Notions" of Optimality in Estimation Procedures}{20}
\contentsline {subsection}{\numberline {5.1}Bayes Optimality}{20}
\contentsline {subsubsection}{\numberline {5.1.1}Selection of Priors $\pi $}{20}
\contentsline {subsubsection}{\numberline {5.1.2}Bayes Estimation for Squared Error Loss}{20}
\contentsline {subsubsection}{\numberline {5.1.3}Bayes Estimation for General Loss Functions}{20}
\contentsline {subsection}{\numberline {5.2}Minimax Optimality}{21}
\contentsline {subsection}{\numberline {5.3}Unbiased Optimality}{21}
\contentsline {subsubsection}{\numberline {5.3.1}Fisher Information (Matrix, or Value)}{21}
\contentsline {section}{\numberline {6}Chapter 4: Hypothesis Testing and Confidence Regions}{22}
\contentsline {section}{\numberline {7}Chapter 5: Asymptotic Approximations}{23}
\contentsline {subsection}{\numberline {7.1}Examples:}{23}
\contentsline {subsubsection}{\numberline {7.1.1}Example 1: Risk of the Median}{23}
\contentsline {section}{\numberline {8}Inference in Multiparameters}{24}
\contentsline {subsection}{\numberline {8.1}Inference for Gaussian Linear Models}{24}
\contentsline {subsubsection}{\numberline {8.1.1}One-Sample Location}{24}
\contentsline {subsection}{\numberline {8.2}Canonical Form of the Gaussian Linear Model}{24}
\contentsline {subsection}{\numberline {8.3}Estimation for Gaussian Linear Models Parameters}{24}
\contentsline {subsubsection}{\numberline {8.3.1}Residual sum of squares (RSS)}{24}
\contentsline {subsubsection}{\numberline {8.3.2}Coefficient of determination ($R^2$) and its problems}{24}
\contentsline {subsubsection}{\numberline {8.3.3}Residuals, Studentized Residuals (i.e. standardized)}{24}
\contentsline {subsection}{\numberline {8.4}References}{24}
\contentsline {section}{\numberline {9}Acknowledgements}{24}
\contentsline {section}{\numberline {10}Supplementary Material}{25}
